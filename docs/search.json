[
  {
    "objectID": "HW5.html#homework5-for-st558",
    "href": "HW5.html#homework5-for-st558",
    "title": "HW5",
    "section": "Homework5 for ST558",
    "text": "Homework5 for ST558"
  },
  {
    "objectID": "HW5.html#task-1-conceptual-questions",
    "href": "HW5.html#task-1-conceptual-questions",
    "title": "HW5",
    "section": "Task 1: Conceptual Questions",
    "text": "Task 1: Conceptual Questions\n\n1. What is the purpose of using cross-validation when fitting a random forest model?\nOverall, cross-validation ensures that the model’s performance is robust, reliable, and generalizable to new data, making it a critical step in the model-building process. In fitting random forest model, 1. cv provides a more accurate estimate of the model’s performance on unseen data compared to a single train/test split. This is because the model is trained and evaluated multiple times on different subsets of the data. 2. it helps in tuning hyperparameters (number of variables) by evaluating the model’s performance across different parameter settings, ensuring the chosen parameters generalize well to new data.\n\n\n2. Describe the bagged tree algorithm.\n\nCreate multiple random bootstrap samples from the original dataset;\nTrain a separate desicion tree for each bootstrap sample;\nFor regression tasks, the predictions from all the trees are averaged to obtain the final prediction. For classification tasks, the final prediction is obtained through majority voting, where the class with the most votes across all trees is chosen.\nThe final prediction for a given input is the aggregated result from all the individual trees. This aggregation helps to reduce the variance of the model and improve its overall performance.\n\n\n\n3. What is meant by a general linear model?\nGeneral Linear Model provides a flexible framework for modeling linear relationships between a response variable and multiple predictor variables, making it a foundational tool in statistical analysis and machine learning.\n\n\n4. When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\nAdding interaction terms to a multiple linear regression model allows for the examination of whether the effect of one predictor variable on the response variable depends on the level of another predictor variable. This can provide a more nuanced and accurate understanding of the relationships between variables.\n\n\n5. Why do we split our data into a training and test set?\nSplitting data into training and test sets is a fundamental practice in machine learning and statistical modeling. The primary purpose is to evaluate the model’s performance on unseen data, ensuring that it generalizes well and is not overfitted to the training data."
  },
  {
    "objectID": "HW5.html#task-2-fitting-models",
    "href": "HW5.html#task-2-fitting-models",
    "title": "HW5",
    "section": "Task 2: Fitting Models",
    "text": "Task 2: Fitting Models\n\nQuick EDA/Data Preparation\n\nCheck on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease.\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyr)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(rpart)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(gbm)\n\nLoaded gbm 2.2.2\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\n\n\nhd_df &lt;- read.csv(\"heart.csv\")\nstr(hd_df)\n\n'data.frame':   918 obs. of  12 variables:\n $ Age           : int  40 49 37 48 54 39 45 54 37 48 ...\n $ Sex           : chr  \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType : chr  \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP     : int  140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol   : int  289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG    : chr  \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR         : int  172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina: chr  \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak       : num  0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ ST_Slope      : chr  \"Up\" \"Flat\" \"Up\" \"Flat\" ...\n $ HeartDisease  : int  0 1 0 1 0 0 0 0 1 0 ...\n\n# do basic summary\npsych::describe(hd_df)\n\n                vars   n   mean     sd median trimmed   mad  min   max range\nAge                1 918  53.51   9.43   54.0   53.71 10.38 28.0  77.0  49.0\nSex*               2 918   1.79   0.41    2.0    1.86  0.00  1.0   2.0   1.0\nChestPainType*     3 918   1.78   0.96    1.0    1.66  0.00  1.0   4.0   3.0\nRestingBP          4 918 132.40  18.51  130.0  131.50 14.83  0.0 200.0 200.0\nCholesterol        5 918 198.80 109.38  223.0  204.41 68.20  0.0 603.0 603.0\nFastingBS          6 918   0.23   0.42    0.0    0.17  0.00  0.0   1.0   1.0\nRestingECG*        7 918   1.99   0.63    2.0    1.99  0.00  1.0   3.0   2.0\nMaxHR              8 918 136.81  25.46  138.0  137.23 26.69 60.0 202.0 142.0\nExerciseAngina*    9 918   1.40   0.49    1.0    1.38  0.00  1.0   2.0   1.0\nOldpeak           10 918   0.89   1.07    0.6    0.74  0.89 -2.6   6.2   8.8\nST_Slope*         11 918   2.36   0.61    2.0    2.41  0.00  1.0   3.0   2.0\nHeartDisease      12 918   0.55   0.50    1.0    0.57  0.00  0.0   1.0   1.0\n                 skew kurtosis   se\nAge             -0.20    -0.40 0.31\nSex*            -1.42     0.02 0.01\nChestPainType*   0.79    -0.72 0.03\nRestingBP        0.18     3.23 0.61\nCholesterol     -0.61     0.10 3.61\nFastingBS        1.26    -0.41 0.01\nRestingECG*      0.01    -0.50 0.02\nMaxHR           -0.14    -0.46 0.84\nExerciseAngina*  0.39    -1.85 0.02\nOldpeak          1.02     1.18 0.04\nST_Slope*       -0.38    -0.67 0.02\nHeartDisease    -0.21    -1.96 0.02\n\n# check missing value\nsum_na &lt;- function(column) {\n  sum(is.na(column))\n}\n\nna_counts &lt;- hd_df |&gt;\n  summarise(across(everything(), sum_na))\nna_counts\n\n  Age Sex ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n1   0   0             0         0           0         0          0     0\n  ExerciseAngina Oldpeak ST_Slope HeartDisease\n1              0       0        0            0\n\n# remove odd observations which RestingBP and Cholesterol = 0\n\nhd_df &lt;- hd_df |&gt;\n  filter(RestingBP &gt; 0 & Cholesterol &gt; 0)\n\n\nCreate a new variable that is a factor version of the HeartDisease variable. Remove the ST_Slope variable and the original HeartDisease variable.\n\n\nhd_df &lt;- hd_df |&gt;\n  mutate(HeartDiseaseF = as.factor(HeartDisease)) |&gt;\n  select(-HeartDisease, -ST_Slope)\n\n\nCreate dummy columns corresponding to the values of Sex, ExerciseAngina, ChestPainType, and RestingECG.\n\n\ndummies &lt;- dummyVars( ~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = hd_df)\ndummy_col &lt;- data.frame(predict(dummies, newdata = hd_df))\n\n# add dummies columns to dataset\n\nhd_df_dummy &lt;- bind_cols(hd_df, dummy_col) |&gt;\n  select(-Sex, -ExerciseAngina, -ChestPainType, -RestingECG)\nstr(hd_df_dummy)\n\n'data.frame':   746 obs. of  18 variables:\n $ Age             : int  40 49 37 48 54 39 45 54 37 48 ...\n $ RestingBP       : int  140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol     : int  289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS       : int  0 0 0 0 0 0 0 0 0 0 ...\n $ MaxHR           : int  172 156 98 108 122 170 170 142 130 120 ...\n $ Oldpeak         : num  0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDiseaseF   : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ...\n $ SexF            : num  0 1 0 1 0 0 1 0 0 1 ...\n $ SexM            : num  1 0 1 0 1 1 0 1 1 0 ...\n $ ExerciseAnginaN : num  1 1 1 0 1 1 1 1 0 1 ...\n $ ExerciseAnginaY : num  0 0 0 1 0 0 0 0 1 0 ...\n $ ChestPainTypeASY: num  0 0 0 1 0 0 0 0 1 0 ...\n $ ChestPainTypeATA: num  1 0 1 0 0 0 1 1 0 1 ...\n $ ChestPainTypeNAP: num  0 1 0 0 1 1 0 0 0 0 ...\n $ ChestPainTypeTA : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGLVH   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGNormal: num  1 1 0 1 1 1 1 1 1 1 ...\n $ RestingECGST    : num  0 0 1 0 0 0 0 0 0 0 ...\n\n\n\n\nSplit the data\nIn this part, I will split the data into training and test sets at 80/20.\n\nset.seed(3033)\nintrain &lt;- createDataPartition(y = hd_df$Age, p= 0.8, list = FALSE)\ntraining &lt;- hd_df[intrain,]\ntesting &lt;- hd_df[-intrain,]\ntraining_dum &lt;- hd_df_dummy[intrain,]\ntesting_dum &lt;- hd_df_dummy[-intrain,]\n\n\n\nkNN\nIn this part, I will fit a kNN model. Firstly, I will train the kNN model using repeated 10 fold cross-validation. Then, the best tunning K will be selected, and lastly, the testing data will be used to evaluate the model performance.\n\ntunningK &lt;- c(1:40)\nknn_fit &lt;- train(HeartDiseaseF ~., \n                 data = training_dum, \n                 method = \"knn\",\n                 trControl= trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = data.frame(k = tunningK))\n\nknn_fit\n\nk-Nearest Neighbors \n\n598 samples\n 17 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (17), scaled (17) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 539, 538, 538, 538, 539, 538, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7635876  0.5262569\n   2  0.7652542  0.5289339\n   3  0.8054237  0.6099913\n   4  0.8093032  0.6180520\n   5  0.8232580  0.6459357\n   6  0.8115443  0.6225206\n   7  0.8199153  0.6390655\n   8  0.8199341  0.6392235\n   9  0.8221186  0.6434275\n  10  0.8254520  0.6502833\n  11  0.8293691  0.6579164\n  12  0.8311111  0.6615789\n  13  0.8322034  0.6635718\n  14  0.8289077  0.6569642\n  15  0.8261111  0.6512645\n  16  0.8272505  0.6535044\n  17  0.8216478  0.6424468\n  18  0.8249812  0.6491147\n  19  0.8244256  0.6479710\n  20  0.8165913  0.6324558\n  21  0.8210640  0.6413323\n  22  0.8221940  0.6436360\n  23  0.8199718  0.6391284\n  24  0.8216478  0.6425608\n  25  0.8194068  0.6380694\n  26  0.8132957  0.6259415\n  27  0.8155179  0.6303220\n  28  0.8149435  0.6293590\n  29  0.8155273  0.6305751\n  30  0.8143974  0.6283219\n  31  0.8166196  0.6329585\n  32  0.8188512  0.6374477\n  33  0.8177307  0.6351885\n  34  0.8216290  0.6430164\n  35  0.8210640  0.6419000\n  36  0.8216196  0.6430362\n  37  0.8221751  0.6440810\n  38  0.8227213  0.6452132\n  39  0.8216102  0.6428795\n  40  0.8232768  0.6461933\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 13.\n\n# check the testing data\nknn_test_pred &lt;- predict(knn_fit, newdata = testing_dum)\nconfusionMatrix(knn_test_pred, testing_dum$HeartDiseaseF)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 70 24\n         1 11 43\n                                          \n               Accuracy : 0.7635          \n                 95% CI : (0.6868, 0.8294)\n    No Information Rate : 0.5473          \n    P-Value [Acc &gt; NIR] : 4.11e-08        \n                                          \n                  Kappa : 0.5146          \n                                          \n Mcnemar's Test P-Value : 0.04252         \n                                          \n            Sensitivity : 0.8642          \n            Specificity : 0.6418          \n         Pos Pred Value : 0.7447          \n         Neg Pred Value : 0.7963          \n             Prevalence : 0.5473          \n         Detection Rate : 0.4730          \n   Detection Prevalence : 0.6351          \n      Balanced Accuracy : 0.7530          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nFrom the kNN model, the final value used for the model was k = 13. The accuracy is 0.7635.\n\n\nLogistic Regression\n\nlr_model1 &lt;- train(HeartDiseaseF ~., \n                 data = training, \n                 method = \"glm\",\n                 trControl= trainControl(method = \"repeatedcv\",\n                                         number = 10, repeats = 3),\n                 family = binomial)\n\nlr_model2 &lt;- train(HeartDiseaseF ~ Cholesterol + FastingBS + RestingECG + MaxHR + ExerciseAngina, \n                 data = training, \n                 method = \"glm\",\n                 trControl= trainControl(method = \"repeatedcv\",\n                                         number = 10, repeats = 3),\n                 family = binomial)\n\nlr_model3 &lt;- train(HeartDiseaseF ~ ChestPainType + Cholesterol + FastingBS + MaxHR + ExerciseAngina + Oldpeak, \n                 data = training, \n                 method = \"glm\",\n                 trControl= trainControl(method = \"repeatedcv\",\n                                         number = 10, repeats = 3),\n                 family = binomial)\nsummary(lr_model1)\n\n\nCall:\nNULL\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -3.855828   1.762247  -2.188  0.02867 *  \nAge               0.023488   0.015613   1.504  0.13248    \nSexM              1.624459   0.319432   5.085 3.67e-07 ***\nChestPainTypeATA -2.085584   0.377293  -5.528 3.24e-08 ***\nChestPainTypeNAP -1.320801   0.298649  -4.423 9.75e-06 ***\nChestPainTypeTA  -1.469439   0.515062  -2.853  0.00433 ** \nRestingBP         0.009208   0.007439   1.238  0.21578    \nCholesterol       0.005336   0.002022   2.638  0.00833 ** \nFastingBS         0.709915   0.343783   2.065  0.03892 *  \nRestingECGNormal  0.013437   0.307658   0.044  0.96516    \nRestingECGST     -0.202303   0.413550  -0.489  0.62471    \nMaxHR            -0.013038   0.005921  -2.202  0.02767 *  \nExerciseAnginaY   1.363005   0.275174   4.953 7.30e-07 ***\nOldpeak           0.861284   0.143357   6.008 1.88e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 828.34  on 597  degrees of freedom\nResidual deviance: 429.74  on 584  degrees of freedom\nAIC: 457.74\n\nNumber of Fisher Scoring iterations: 5\n\nsummary(lr_model2)\n\n\nCall:\nNULL\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       1.907720   0.842087   2.265  0.02348 *  \nCholesterol       0.003356   0.001689   1.987  0.04687 *  \nFastingBS         0.788896   0.292932   2.693  0.00708 ** \nRestingECGNormal -0.426130   0.254593  -1.674  0.09418 .  \nRestingECGST     -0.445654   0.340310  -1.310  0.19035    \nMaxHR            -0.025140   0.004788  -5.251 1.52e-07 ***\nExerciseAnginaY   2.376332   0.225783  10.525  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 828.34  on 597  degrees of freedom\nResidual deviance: 570.01  on 591  degrees of freedom\nAIC: 584.01\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(lr_model3)\n\n\nCall:\nNULL\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.777380   0.883134   0.880  0.37872    \nChestPainTypeATA -2.073276   0.365590  -5.671 1.42e-08 ***\nChestPainTypeNAP -1.327726   0.282201  -4.705 2.54e-06 ***\nChestPainTypeTA  -1.206304   0.497273  -2.426  0.01527 *  \nCholesterol       0.003435   0.001884   1.823  0.06823 .  \nFastingBS         1.003539   0.321935   3.117  0.00183 ** \nMaxHR            -0.017172   0.005319  -3.228  0.00125 ** \nExerciseAnginaY   1.385511   0.262277   5.283 1.27e-07 ***\nOldpeak           0.890196   0.137149   6.491 8.54e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 828.34  on 597  degrees of freedom\nResidual deviance: 460.98  on 589  degrees of freedom\nAIC: 478.98\n\nNumber of Fisher Scoring iterations: 5\n\n# choose the best model, lr_model1, according to smaller AIC.\n\n# check the testing data\nlr_test_pred &lt;- predict(lr_model1, newdata = testing)\nconfusionMatrix(lr_test_pred, testing$HeartDiseaseF)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 68 23\n         1 13 44\n                                          \n               Accuracy : 0.7568          \n                 95% CI : (0.6795, 0.8235)\n    No Information Rate : 0.5473          \n    P-Value [Acc &gt; NIR] : 1.09e-07        \n                                          \n                  Kappa : 0.5027          \n                                          \n Mcnemar's Test P-Value : 0.1336          \n                                          \n            Sensitivity : 0.8395          \n            Specificity : 0.6567          \n         Pos Pred Value : 0.7473          \n         Neg Pred Value : 0.7719          \n             Prevalence : 0.5473          \n         Detection Rate : 0.4595          \n   Detection Prevalence : 0.6149          \n      Balanced Accuracy : 0.7481          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nAccording to the AIC value of the 3 logistic regression models, lr_model1 was chosen as the best model. Use the testing data set, the accuracy of the model is 0.7568.\n\n\nTree Models\n\nClassification tree model\n\n\nset.seed(1001)\nct_fit &lt;- train(HeartDiseaseF ~ ., \n                 data = training, \n                 method = \"rpart\",\n                 trControl= trainControl(method = \"repeatedcv\",\n                                         number = 10, repeats = 3),\n                 tuneLength = 100)\n\n\n# check the testing data\nct_test_pred &lt;- predict(ct_fit, newdata = testing)\nconfusionMatrix(table(ct_test_pred, testing$HeartDiseaseF))\n\nConfusion Matrix and Statistics\n\n            \nct_test_pred  0  1\n           0 69 27\n           1 12 40\n                                          \n               Accuracy : 0.7365          \n                 95% CI : (0.6578, 0.8054)\n    No Information Rate : 0.5473          \n    P-Value [Acc &gt; NIR] : 1.648e-06       \n                                          \n                  Kappa : 0.4577          \n                                          \n Mcnemar's Test P-Value : 0.02497         \n                                          \n            Sensitivity : 0.8519          \n            Specificity : 0.5970          \n         Pos Pred Value : 0.7187          \n         Neg Pred Value : 0.7692          \n             Prevalence : 0.5473          \n         Detection Rate : 0.4662          \n   Detection Prevalence : 0.6486          \n      Balanced Accuracy : 0.7244          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nThe classification tree model return the testing data set at accuracy of 0.7365.\n\nRandom forest model\n\n\nset.seed(1001)\ntuneGrid &lt;- expand.grid(mtry = c(1:10))\nrf_fit &lt;- train(HeartDiseaseF ~ ., \n                 data = training, \n                 method = \"rf\",\n                 trControl= trainControl(method = \"repeatedcv\",\n                                         number = 10, repeats = 3),\n                 tuneGrid = tuneGrid)\n#train final model\nfinal_rf &lt;- randomForest(HeartDiseaseF ~ .,\n                         data = training, \n                         mtry = rf_fit$bestTune$mtry)\n\n# check the testing data\nrf_test_pred &lt;- predict(final_rf, newdata = testing)\nconfusionMatrix(table(rf_test_pred, testing$HeartDiseaseF))\n\nConfusion Matrix and Statistics\n\n            \nrf_test_pred  0  1\n           0 70 23\n           1 11 44\n                                         \n               Accuracy : 0.7703         \n                 95% CI : (0.694, 0.8354)\n    No Information Rate : 0.5473         \n    P-Value [Acc &gt; NIR] : 1.494e-08      \n                                         \n                  Kappa : 0.5291         \n                                         \n Mcnemar's Test P-Value : 0.05923        \n                                         \n            Sensitivity : 0.8642         \n            Specificity : 0.6567         \n         Pos Pred Value : 0.7527         \n         Neg Pred Value : 0.8000         \n             Prevalence : 0.5473         \n         Detection Rate : 0.4730         \n   Detection Prevalence : 0.6284         \n      Balanced Accuracy : 0.7605         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nThe random forest model return the testing data set at accuracy of 0.7568.\n\nBoosted tree model\n\n\nset.seed(1001)\ngr &lt;- expand.grid(shrinkage = 0.1,\n                  interaction.depth = c(1,2,3),\n                  n.trees = c(25, 50, 100, 200),\n                  n.minobsinnode = 10)\n\ngbm_fit &lt;- train(HeartDiseaseF ~ ., \n                 data = training, \n                 method = \"gbm\",\n                 trControl= trainControl(method = \"repeatedcv\",\n                                         number = 10, repeats = 3),\n                 tuneGrid = gr,\n                 verbose = FALSE)\n\n# check the testing data\ngbm_test_pred &lt;- predict(gbm_fit, newdata = testing)\nconfusionMatrix(table(gbm_test_pred, testing$HeartDiseaseF))\n\nConfusion Matrix and Statistics\n\n             \ngbm_test_pred  0  1\n            0 67 22\n            1 14 45\n                                          \n               Accuracy : 0.7568          \n                 95% CI : (0.6795, 0.8235)\n    No Information Rate : 0.5473          \n    P-Value [Acc &gt; NIR] : 1.09e-07        \n                                          \n                  Kappa : 0.504           \n                                          \n Mcnemar's Test P-Value : 0.2433          \n                                          \n            Sensitivity : 0.8272          \n            Specificity : 0.6716          \n         Pos Pred Value : 0.7528          \n         Neg Pred Value : 0.7627          \n             Prevalence : 0.5473          \n         Detection Rate : 0.4527          \n   Detection Prevalence : 0.6014          \n      Balanced Accuracy : 0.7494          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nThe GBM model return the testing data set at accuracy of 0.75.\n\n\nWrap up\nAs shown above, the accuracy on the test data set of each model are: 1. kNN: 0.7635 2. Logistic regression:0.7568 3. Classification tree: 0.7365 4. Random forest: 0.7568 5. Boosted tree: 0.75\nWe could conclude that the best model is kNN."
  }
]
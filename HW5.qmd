---
title: "HW5"
format: html
editor: visual
---

## Name: Xingwang Yu

## Homework5 for ST558

## Task 1: Conceptual Questions

### 1. What is the purpose of using cross-validation when fitting a random forest model?

Overall, cross-validation ensures that the model's performance is robust, reliable, and generalizable to new data, making it a critical step in the model-building process. In fitting random forest model, 1. cv provides a more accurate estimate of the model's performance on unseen data compared to a single train/test split. This is because the model is trained and evaluated multiple times on different subsets of the data. 2. it helps in tuning hyperparameters (number of variables) by evaluating the model's performance across different parameter settings, ensuring the chosen parameters generalize well to new data.

### 2. Describe the bagged tree algorithm.

1.  Create multiple random bootstrap samples from the original dataset;
2.  Train a separate desicion tree for each bootstrap sample;
3.  For regression tasks, the predictions from all the trees are averaged to obtain the final prediction. For classification tasks, the final prediction is obtained through majority voting, where the class with the most votes across all trees is chosen.
4.  The final prediction for a given input is the aggregated result from all the individual trees. This aggregation helps to reduce the variance of the model and improve its overall performance.

### 3. What is meant by a general linear model?

General Linear Model provides a flexible framework for modeling linear relationships between a response variable and multiple predictor variables, making it a foundational tool in statistical analysis and machine learning.

### 4. When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

Adding interaction terms to a multiple linear regression model allows for the examination of whether the effect of one predictor variable on the response variable depends on the level of another predictor variable. This can provide a more nuanced and accurate understanding of the relationships between variables.

### 5. Why do we split our data into a training and test set?

Splitting data into training and test sets is a fundamental practice in machine learning and statistical modeling. The primary purpose is to evaluate the model's performance on unseen data, ensuring that it generalizes well and is not overfitted to the training data.

## Task 2: Fitting Models

### Quick EDA/Data Preparation

1.  Check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease.

```{r setup, warning=FALSE}
library(tidyverse)
library(tidyr)
library(caret)
library(rpart)
library(randomForest)
library(gbm)
```

```{r}
hd_df <- read.csv("heart.csv")
str(hd_df)

# do basic summary
psych::describe(hd_df)

# check missing value
sum_na <- function(column) {
  sum(is.na(column))
}

na_counts <- hd_df |>
  summarise(across(everything(), sum_na))
na_counts

# remove odd observations which RestingBP and Cholesterol = 0

hd_df <- hd_df |>
  filter(RestingBP > 0 & Cholesterol > 0)

```

2.  Create a new variable that is a factor version of the HeartDisease variable. Remove the ST_Slope variable and the original HeartDisease variable.

```{r}
hd_df <- hd_df |>
  mutate(HeartDiseaseF = as.factor(HeartDisease)) |>
  select(-HeartDisease, -ST_Slope)
```

3.  Create dummy columns corresponding to the values of Sex, ExerciseAngina, ChestPainType, and RestingECG.

```{r}
dummies <- dummyVars( ~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = hd_df)
dummy_col <- data.frame(predict(dummies, newdata = hd_df))

# add dummies columns to dataset

hd_df_dummy <- bind_cols(hd_df, dummy_col) |>
  select(-Sex, -ExerciseAngina, -ChestPainType, -RestingECG)
str(hd_df_dummy)
```

### Split the data

In this part, I will split the data into training and test sets at 80/20.

```{r}
set.seed(3033)
intrain <- createDataPartition(y = hd_df$Age, p= 0.8, list = FALSE)
training <- hd_df[intrain,]
testing <- hd_df[-intrain,]
training_dum <- hd_df_dummy[intrain,]
testing_dum <- hd_df_dummy[-intrain,]
```

### kNN

In this part, I will fit a kNN model. Firstly, I will train the kNN model using repeated 10 fold cross-validation. Then, the best tunning K will be selected, and lastly, the testing data will be used to evaluate the model performance.

```{r}
tunningK <- c(1:40)
knn_fit <- train(HeartDiseaseF ~., 
                 data = training_dum, 
                 method = "knn",
                 trControl= trainControl(method = "repeatedcv", number = 10, repeats = 3),
                 preProcess = c("center", "scale"),
                 tuneGrid = data.frame(k = tunningK))

knn_fit

# check the testing data
knn_test_pred <- predict(knn_fit, newdata = testing_dum)
confusionMatrix(knn_test_pred, testing_dum$HeartDiseaseF)
```

From the kNN model, the final value used for the model was k = 13. The accuracy is 0.7635.

### Logistic Regression

```{r}
lr_model1 <- train(HeartDiseaseF ~., 
                 data = training, 
                 method = "glm",
                 trControl= trainControl(method = "repeatedcv",
                                         number = 10, repeats = 3),
                 family = binomial)

lr_model2 <- train(HeartDiseaseF ~ Cholesterol + FastingBS + RestingECG + MaxHR + ExerciseAngina, 
                 data = training, 
                 method = "glm",
                 trControl= trainControl(method = "repeatedcv",
                                         number = 10, repeats = 3),
                 family = binomial)

lr_model3 <- train(HeartDiseaseF ~ ChestPainType + Cholesterol + FastingBS + MaxHR + ExerciseAngina + Oldpeak, 
                 data = training, 
                 method = "glm",
                 trControl= trainControl(method = "repeatedcv",
                                         number = 10, repeats = 3),
                 family = binomial)
summary(lr_model1)
summary(lr_model2)
summary(lr_model3)

# choose the best model, lr_model1, according to smaller AIC.

# check the testing data
lr_test_pred <- predict(lr_model1, newdata = testing)
confusionMatrix(lr_test_pred, testing$HeartDiseaseF)
```

According to the AIC value of the 3 logistic regression models, lr_model1 was chosen as the best model. Use the testing data set, the accuracy of the model is 0.7568.

### Tree Models

1.  Classification tree model

```{r}
set.seed(1001)
ct_fit <- train(HeartDiseaseF ~ ., 
                 data = training, 
                 method = "rpart",
                 trControl= trainControl(method = "repeatedcv",
                                         number = 10, repeats = 3),
                 tuneLength = 100)


# check the testing data
ct_test_pred <- predict(ct_fit, newdata = testing)
confusionMatrix(table(ct_test_pred, testing$HeartDiseaseF))
```

The classification tree model return the testing data set at accuracy of 0.7365.

2.  Random forest model

```{r}
set.seed(1001)
tuneGrid <- expand.grid(mtry = c(1:10))
rf_fit <- train(HeartDiseaseF ~ ., 
                 data = training, 
                 method = "rf",
                 trControl= trainControl(method = "repeatedcv",
                                         number = 10, repeats = 3),
                 tuneGrid = tuneGrid)
#train final model
final_rf <- randomForest(HeartDiseaseF ~ .,
                         data = training, 
                         mtry = rf_fit$bestTune$mtry)

# check the testing data
rf_test_pred <- predict(final_rf, newdata = testing)
confusionMatrix(table(rf_test_pred, testing$HeartDiseaseF))
```

The random forest model return the testing data set at accuracy of 0.7568.

3.  Boosted tree model

```{r}
set.seed(1001)
gr <- expand.grid(shrinkage = 0.1,
                  interaction.depth = c(1,2,3),
                  n.trees = c(25, 50, 100, 200),
                  n.minobsinnode = 10)

gbm_fit <- train(HeartDiseaseF ~ ., 
                 data = training, 
                 method = "gbm",
                 trControl= trainControl(method = "repeatedcv",
                                         number = 10, repeats = 3),
                 tuneGrid = gr,
                 verbose = FALSE)

# check the testing data
gbm_test_pred <- predict(gbm_fit, newdata = testing)
confusionMatrix(table(gbm_test_pred, testing$HeartDiseaseF))
```

The GBM model return the testing data set at accuracy of 0.75.

### Wrap up

As shown above, the accuracy on the test data set of each model are: 1. kNN: 0.7635 2. Logistic regression:0.7568 3. Classification tree: 0.7365 4. Random forest: 0.7568 5. Boosted tree: 0.75

We could conclude that the best model is kNN.
